import logging
import sys
import os
import pandas as pd
from src.data_ingestion import ConflictDownloader, WorldBankFetcher
from src.data_processing import DataProcessor
from src.econometrics import EconometricModeler
from src.ml_models import ConflictML
from src.reporting import Reporter

# Configure logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', handlers=[
    logging.StreamHandler(sys.stdout),
    logging.FileHandler("analysis.log")
])
logger = logging.getLogger(__name__)

def main():
    logger.info("Starting Global Analysis Pipeline...")

    # 1. Data Ingestion & 2. Data Processing
    if os.path.exists("processed_global_data.csv"):
        logger.info("Loading existing processed data...")
        clean_df = pd.read_csv("processed_global_data.csv")
    else:
        logger.info("--- Step 1: Data Ingestion ---")
        try:
            # Conflict Data
            cd = ConflictDownloader()
            conflict_df = cd.process_cow_data()
            logger.info(f"Conflict Data Downloaded: {conflict_df.shape}")

            # World Bank Data
            wb = WorldBankFetcher()
            meta_df = wb.fetch_metadata()
            econ_df = wb.fetch_indicators(start_year=1990, end_year=2024)
            logger.info(f"Economic Data Downloaded: {econ_df.shape}")

        except Exception as e:
            logger.error(f"Ingestion failed: {e}")
            return

        # 2. Data Processing
        logger.info("--- Step 2: Data Processing ---")
        dp = DataProcessor(conflict_df, econ_df, meta_df)
        dp.merge_data()
        dp.impute_missing_values()
        dp.engineer_features()
        clean_df = dp.get_clean_panel()

        # Save processed data
        clean_df.to_csv("processed_global_data.csv", index=False)
        logger.info(f"Processed Data Saved: {clean_df.shape}")

    # 3. Econometric Analysis
    logger.info("--- Step 3: Econometric Analysis ---")
    em = EconometricModeler(clean_df)

    # New Analysis Methods
    res_horse_race = em.run_horse_race_model()
    hierarchy_results = em.run_hierarchy_test()
    res_event = em.run_event_study()
    hetero_results = em.run_heterogeneity_models()

    # Consolidate for Report
    all_results = {
        'Horse_Race_Institutions_vs_Volatility': res_horse_race,
        'Event_Study_Persistence': res_event,
        **hierarchy_results,
        **hetero_results
    }

    em.save_results(all_results)

    # 4. Machine Learning & Counterfactuals
    logger.info("--- Step 4: Machine Learning & Counterfactuals ---")
    ml = ConflictML(clean_df)

    # XGBoost
    xgb_model, feature_names, rmse = ml.train_xgboost()

    # Counterfactual Clustering
    counterfactual_res = ml.run_counterfactual_clustering()

    # Standard Clustering (for plotting if needed)
    res_traj = ml.cluster_recovery_trajectories()
    if res_traj:
        clusters_df, kmeans_model = res_traj
    else:
        clusters_df, kmeans_model = None, None

    res_trade = ml.cluster_trade_patterns()
    if res_trade:
        trade_clusters_df, trade_kmeans = res_trade
    else:
        trade_clusters_df, trade_kmeans = None, None

    # 5. Reporting
    logger.info("--- Step 5: Reporting ---")
    reporter = Reporter(clean_df)

    # Comprehensive Plots
    reporter.plot_data_coverage()
    reporter.plot_conflict_structure()
    reporter.plot_gdp_variance()
    reporter.plot_event_study()
    reporter.plot_feature_importance(xgb_model, feature_names)
    reporter.plot_recovery_clusters(clusters_df)
    reporter.plot_trade_clusters(trade_clusters_df)
    reporter.plot_trade_spillover()
    reporter.plot_currency_volatility()
    reporter.plot_inflation_instability()

    # Append Counterfactual Info to summary
    cf_summary = ""
    if counterfactual_res:
        cf_summary = f"\n\n## Counterfactual Analysis\n"
        cf_summary += f"Simulating a reduction in FX Volatility for high-risk countries resulted in:\n"
        cf_summary += f"- **{counterfactual_res.get('pct_migrated', 0):.2f}%** of countries migrating from the 'Conflict/Volatility Trap' cluster to a more stable regime.\n"
        cf_summary += f"- This confirms that volatility reduction is a sufficient condition for regime change in a significant subset of cases."

    # Generate Report
    # Pass 'all_results' summary string?
    # For now, we rely on REGRESSION_TABLE.tex which is generated by em.save_results()
    # We pass the consolidated dict keys as a string summary for the MD file

    model_summary_str = "\n".join([f"- {k}: Completed" for k in all_results.keys()])

    reporter.generate_report_md(model_summary_str + cf_summary, ml_rmse=rmse)
    reporter.generate_latex()

    logger.info("Pipeline Completed Successfully.")

if __name__ == "__main__":
    main()
